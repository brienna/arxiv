{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse preprint full-texts\n",
    "\n",
    "Contents:\n",
    "1. Introduction\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if downloaded .tex files contain fulltext. Some are just supplementary files for the submission.  If they contain \\documentclass. \n",
    "Each submission folder should contain one \\documentclass file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, requests, subprocess, glob, multiprocessing, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each submission folder may contain 1 or more .tex files. We need to identify which .tex file is the full-text preprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprints():\n",
    "    base_path = 'latex'\n",
    "    submissions = 0\n",
    "    tex_count = 0\n",
    "    empties = []\n",
    "    preprints = []\n",
    "    missed = []\n",
    "\n",
    "    # Walk through tars\n",
    "    for idx, tar_folder in enumerate(os.listdir(base_path)):\n",
    "        tar_path = base_path + '/' + tar_folder\n",
    "        if not os.path.isdir(tar_path):\n",
    "            continue\n",
    "        submission_dirs = os.listdir(tar_path)\n",
    "        submissions += len(submission_dirs)\n",
    "        # Walk through each submission\n",
    "        for submission in submission_dirs:\n",
    "            submission_path = tar_path + '/' + submission\n",
    "            if not os.path.isdir(submission_path):\n",
    "                submissions -= 1\n",
    "                continue\n",
    "            arxiv_id = os.path.basename(submission_path)\n",
    "            texs = glob.glob(submission_path + '/**/*.tex', recursive=True)\n",
    "            tex_count += len(texs)\n",
    "            \n",
    "            # If submission is empty, note & skip\n",
    "            if len(texs) == 0:\n",
    "                empties.append(arxiv_id)\n",
    "                continue\n",
    "            # Otherwise get the preprint\n",
    "            else:\n",
    "                preprint_path = getPreprint(submission_path, texs)\n",
    "                if preprint_path:\n",
    "                    preprints.append(preprint_path)\n",
    "                else:\n",
    "                    missed.append(arxiv_id)\n",
    "                    \n",
    "    print('Submissions: ' + str(submissions))\n",
    "    print('Preprints: ' + str(len(preprints)))\n",
    "    print('Non-empty submissions missing preprints: ' + str(len(missed)))\n",
    "    print('Empty submissions: ' + str(len(empties)))\n",
    "    print('Texs: ' + str(tex_count))\n",
    "    print(str(missed))\n",
    "    \n",
    "    return preprints\n",
    "\n",
    "def getPreprint(submission_path, texs):\n",
    "    preprint = None\n",
    "    \n",
    "    # If submission contains only one file, this is the preprint\n",
    "    if len(texs) == 1:\n",
    "        preprint = texs[0]\n",
    "    # If submission contains ms.tex or main.tex, this is the preprint\n",
    "    elif 'ms.tex' in texs:\n",
    "        preprint = submission_path + '/' + 'ms.tex'\n",
    "    elif 'main.tex' in texs:\n",
    "        preprint = submission_path + '/' + 'main.tex'\n",
    "    # Otherwise, iterate through each .tex looking for \\documentclass or \\documentstyle\n",
    "    else: \n",
    "        for tex_path in texs: \n",
    "            with open(tex_path, 'rb') as f: \n",
    "                data = f.readlines()\n",
    "                r = re.compile(b'(.*\\\\\\\\documentclass.*)|(.*\\\\\\\\documentstyle.*)')\n",
    "                if len(list(filter(r.match, data))) > 0:\n",
    "                    preprint = tex_path\n",
    "                    break\n",
    "    \n",
    "    return preprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissions: 89908\n",
      "Preprints: 89630\n",
      "Non-empty submissions missing preprints: 7\n",
      "Empty submissions: 271\n",
      "Texs: 125484\n",
      "['1105.1087', '1211.4277', '1304.7762', '1308.6483', '1409.3422', '1606.06791', '1607.01189']\n"
     ]
    }
   ],
   "source": [
    "preprints = get_preprints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89630"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arxiv-vanity is unable to render these 7 papers as well. I'll just leave them for now.\n",
    "\n",
    "Convert each preprint via multiprocessing.\n",
    "\n",
    "Before you can begin multiprocessing, you need to pick which sections of code to multiprocess. These sections of code must meet the following criteria:\n",
    "\n",
    "1. Must not be reliant on previous outcomes. True.\n",
    "\n",
    "2. Does not need to be executed in a particular order. True.\n",
    "\n",
    "3. Does not return anything that would need to be accessed later in the code. True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3737/stats_ms4_pdf.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3779/preprint.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0722/manuscrip3.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7015/DA+DAH-archive.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7015/DA+DAH-archive.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7019/BD-arXiv.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3737/stats_ms4_pdf.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3740/nstarB-f2.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3779/preprint.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3780/ms.tex to XML\n",
      "ms.tex has already been converted.\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3781/bhnodisk.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7019/BD-arXiv.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7021/b1ePaperFinal+update.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1107_001/1107.0722/manuscrip3.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0725/AA_bcg3.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3781/bhnodisk.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3782/sausagePR.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3740/nstarB-f2.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3750/Lundqvist_pwn0540_corrected.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1107_001/1107.0725/AA_bcg3.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0726/Pcomb.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3782/sausagePR.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3783/ms.tex to XML\n",
      "ms.tex has already been converted.\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3786/15982.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7021/b1ePaperFinal+update.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7023/18211.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3750/Lundqvist_pwn0540_corrected.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3764/ms.tex to XML\n",
      "ms.tex has already been converted.\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3790/buerzleetal.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3786/15982.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3788/ms.tex to XML\n",
      "ms.tex has already been converted.\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3799/facal-airfly.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7023/18211.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7026/june12_nov23.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3799/facal-airfly.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3800/ms.tex to XML\n",
      "ms.tex has already been converted.\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3801/KLK_v3_final_1901.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7026/june12_nov23.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7027/1111.7027.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1107_001/1107.0726/Pcomb.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0727/mn_resub.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3790/buerzleetal.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3791/hdisc_grl.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3801/KLK_v3_final_1901.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3802/GBT_3feb11.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3791/hdisc_grl.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3797/hh223_astro.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1107_001/1107.0727/mn_resub.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0728/ms_arxiv.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3797/hh223_astro.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3803/body.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3802/GBT_3feb11.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3813/Ford_Alyson.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3813/Ford_Alyson.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3814/PHR1315-6555_astro-ph.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3803/body.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3806/Beck_Crete10.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3806/Beck_Crete10.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3807/gyurky_151Eu_JPG.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1107_001/1107.0728/ms_arxiv.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0729/1107.0729.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3814/PHR1315-6555_astro-ph.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3815/ms.tex to XML\n",
      "ms.tex has already been converted.\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3842/grl_sources_a.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7027/1111.7027.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7030/OFUpaperAA.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3842/grl_sources_a.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3847/ms.tex to XML\n",
      "ms.tex has already been converted.\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3848/beeck_b.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3807/gyurky_151Eu_JPG.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3817/15529.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3848/beeck_b.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3865/15991.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3817/15529.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3818/15254.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7030/OFUpaperAA.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7037/sdhigh_mnras_rev.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7037/sdhigh_mnras_rev.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7039/ShocksIII_sub.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3865/15991.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3866/Xe100_EMbg_v3_pdf.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1107_001/1107.0729/1107.0729.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0731/1107.0731.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3818/15254.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3823/vennes.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3866/Xe100_EMbg_v3_pdf.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3870/0294380.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7039/ShocksIII_sub.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7042/ms.tex to XML\n",
      "ms.tex has already been converted.\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7068/modulation-2.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7068/modulation-2.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7071/BrakingApJ.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1107_001/1107.0731/1107.0731.tex to XML\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0732/1107.0732.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3870/0294380.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3875/nsi_hints_v2_arxiv.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3823/vennes.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3828/astroph.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7071/BrakingApJ.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7072/IRFlare_sjschmidt.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3875/nsi_hints_v2_arxiv.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3876/AstroPH_IRlums-finalrevised.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1008_005/1008.3828/astroph.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1008_005/1008.3848/13544.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1107_001/1107.0732/1107.0732.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1107_001/1107.0736/tidal_kozai.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7072/IRFlare_sjschmidt.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7081/krause.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7081/krause.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7084/FGRB.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7084/FGRB.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7092/1111.7092.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1111_009/1111.7092/1111.7092.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1111_009/1111.7110/LP_WISAP.tex to XML\n",
      "--> Conversion complete for latex/arXiv_src_1101_005/1101.3876/AstroPH_IRlums-finalrevised.tex to XML\n",
      "--> Conversion beginning for latex/arXiv_src_1101_005/1101.3880/15519IRAS2a.tex to XML\n"
     ]
    }
   ],
   "source": [
    "def convert_to_xml(tex_path):\n",
    "    '''\n",
    "    Converts TEX file to XML.\n",
    "    '''\n",
    "    \n",
    "    print('--> Conversion beginning for {} to XML'.format(tex_path))\n",
    "    # Convert given .tex to XML\n",
    "    filename, file_extension = os.path.splitext(os.path.basename(tex_path))\n",
    "    xml_path = 'xml/' + filename + '.xml'\n",
    "    \n",
    "    if os.path.exists(xml_path):\n",
    "        print(os.path.basename(tex_path) + ' has already been converted.')\n",
    "    elif file_extension == '.tex':\n",
    "        subprocess.call(['latexml', '--dest=' + xml_path, tex_path])\n",
    "        print('--> Conversion complete for {} to XML'.format(tex_path))\n",
    "\n",
    "def start_conversion():\n",
    "    starttime = time.time()\n",
    "    pool = multiprocessing.Pool(processes=4)\n",
    "    # Specify a timeout so that this pool can be interrupted\n",
    "    pool.map_async(convert_to_xml, preprints).get(9999999)\n",
    "    pool.close()\n",
    "    print('That took {} seconds'.format(time.time() - starttime))\n",
    "\n",
    "start_conversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "If I use `pool.map(convert_to_xml, preprints)`, I cannot kill this. So this is why I used `pool.map_async()` with a timeout specified. \n",
    "https://stackoverflow.com/questions/1408356/keyboard-interrupts-with-pythons-multiprocessing-pool\n",
    "\n",
    "If I ever use pool.map accidentally - this is how to kill the stupid Medusa-spawning workers: https://stackoverflow.com/questions/25415104/kill-python-multiprocessing-pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding tasks...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-bb5596361bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Time taken = {time.time() - start:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-228-bb5596361bb4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mempty_task_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mfull_task_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_task_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUMBER_OF_TASKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running with {PROCESSES} processes!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-228-bb5596361bb4>\u001b[0m in \u001b[0;36madd_tasks\u001b[0;34m(task_queue, number_of_tasks)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adding tasks...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtask_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtask_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Queue {0!r} has been closed\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mFull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PROCESSES = 4\n",
    "\n",
    "def process_tasks(task_queue):\n",
    "    print('Processing tasks...')\n",
    "    while not task_queue.empty():\n",
    "        preprint = task_queue.get()\n",
    "        convert_to_xml(preprint)\n",
    "    return True\n",
    "\n",
    "def add_tasks(task_queue, number_of_tasks):\n",
    "    print('Adding tasks...')\n",
    "    for num in range(len(preprints)):\n",
    "        task_queue.put(preprints[num])\n",
    "    return task_queue\n",
    "\n",
    "def run():\n",
    "    empty_task_queue = multiprocessing.Queue()\n",
    "    full_task_queue = add_tasks(empty_task_queue, NUMBER_OF_TASKS)\n",
    "    processes = []\n",
    "    print(f'Running with {PROCESSES} processes!')\n",
    "    start = time.time()\n",
    "    for n in range(PROCESSES):\n",
    "        p = multiprocessing.Process(\n",
    "            target=process_tasks, args=(full_task_queue,))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    print(f'Time taken = {time.time() - start:.3f}')\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all to XML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt using more cores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is faster to use all 8 cores. So I will do the XML conversion using all 8 cores. \n",
    "\n",
    "First I need to confirm the main file in each repository. \n",
    "- If it doesn't contain a .bbl file, I need to add it to the bbl_lack folder. Later. Set aside and skip.\n",
    "- If it doesn't contain a file, I need to retrieve it again. Later. Set aside and skip. \n",
    "\n",
    "I will look at each submission folder, check xml to see if a file exists with its name. If not, I will go into the submission folder to check each file if it contains \\\\documentclass. If it does, grab it and convert it. Break out of loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the functions, with conversion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have 8 cores: https://superuser.com/questions/1101311/how-many-cores-does-my-mac-have/1101314#1101314\n",
    "4 physical and 4 logical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1010.3382.tar.gz\n"
     ]
    }
   ],
   "source": [
    "def guess_extension_from_headers(h):\n",
    "    \"\"\"\n",
    "    Given headers from an ArXiV e-print response, try and guess what the file\n",
    "    extension should be.\n",
    "    Based on: https://arxiv.org/help/mimetypes\n",
    "    \"\"\"\n",
    "    if h.get('content-type') == 'application/pdf':\n",
    "        return '.pdf'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/postscript':\n",
    "        return '.ps.gz'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/x-eprint-tar':\n",
    "        return '.tar.gz'\n",
    "    # content-encoding is x-gzip but this appears to normally be a lie - it's\n",
    "    # just plain text\n",
    "    if h.get('content-type') == 'application/x-eprint':\n",
    "        return '.tex'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/x-dvi':\n",
    "        return '.dvi.gz'\n",
    "    return None\n",
    "\n",
    "def arxiv_id_to_source_url(arxiv_id):\n",
    "    # This URL is normally a tarball, but sometimes something else.\n",
    "    # ArXiV provides a /src/ URL which always serves up a tarball,\n",
    "    # but if we used this, we'd have to untar the file to figure out\n",
    "    # whether it's renderable or not. By using the /e-print/ endpoint\n",
    "    # we can figure out straight away whether we should bother rendering\n",
    "    # it or not.\n",
    "    # https://arxiv.org/help/mimetypes has more info\n",
    "    return 'https://arxiv.org/e-print/' + arxiv_id\n",
    "\n",
    "def download_source_file(arxiv_id):\n",
    "    \"\"\"\n",
    "    Download the LaTeX source of this paper and returns as ContentFile.\n",
    "    \"\"\"\n",
    "    source_url = arxiv_id_to_source_url(arxiv_id)\n",
    "    res = requests.get(source_url)\n",
    "    res.raise_for_status()\n",
    "    extension = guess_extension_from_headers(res.headers)\n",
    "    if not extension:\n",
    "        raise DownloadError(\"Could not determine file extension from \"\n",
    "                            \"headers: Content-Type: {}; \"\n",
    "                            \"Content-Encoding: {}\".format(\n",
    "                                res.headers.get('content-type'),\n",
    "                                res.headers.get('content-encoding')))\n",
    "    with open(arxiv_id + extension, 'wb+') as f:\n",
    "        f.write(res.content)\n",
    "        print('Created ' + arxiv_id + extension)\n",
    "\n",
    "download_source_file('1010.3382')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
